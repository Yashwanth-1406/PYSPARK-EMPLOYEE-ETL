{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe60iYXvdxMFtrGf5VOM+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashwanth-1406/PYSPARK-EMPLOYEE-ETL/blob/main/ETL_PROCESS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ETL Process –\n",
        "\n",
        "ETL = Extract → Transform → Load\n",
        "\n",
        "This is the backbone of all data pipelines in service-based companies in India or anywhere. Basically how raw data becomes usable, clean data for analytics, dashboards, and reports.\n",
        "\n",
        "---\n",
        "\n",
        "1. Extract (E)\n",
        "\n",
        "* What it is: Pulling data from all the places it exists.\n",
        "* Sources:\n",
        "\n",
        "  * Databases (MySQL, Oracle, Postgres)\n",
        "  * Files (CSV, Excel, JSON, logs)\n",
        "  * APIs or streaming sources (Kafka, app logs)\n",
        "* What can go wrong:\n",
        "\n",
        "  * Different formats\n",
        "  * Missing data\n",
        "  * Large volume\n",
        "  * Slow refresh rates\n",
        "* Goal: Get all data together into one place to work with it.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Transform (T)\n",
        "\n",
        "* What it is: Cleaning, standardizing, enriching, and structuring the data.\n",
        "* Key steps:\n",
        "\n",
        "  * Cleaning → Fix missing values, remove duplicates, correct wrong formats.\n",
        "  * Standardizing → Make everything uniform (dates, phone numbers, names).\n",
        "  * Joining → Combine multiple tables (e.g., Employee + Department + Salary).\n",
        "  * Deriving new columns→ Add useful info like Tax = 10% of salary, or categorize employees.\n",
        "  * Aggregations → Calculate sums, averages, min, max, counts, etc.\n",
        "  * Complex handling → Flatten JSON, explode arrays, collect lists, window functions, UDFs.\n",
        "* Goal: Turn messy raw data into **structured, trustworthy, ready-to-use data.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Load (L)**\n",
        "\n",
        "* **What it is**: Saving the cleaned and structured data in a place where others can use it.\n",
        "* **Destinations**:\n",
        "\n",
        "  * Databases (SQL, NoSQL)\n",
        "  * Data warehouses (Snowflake, BigQuery, Redshift)\n",
        "  * Cloud storage (AWS S3, Azure Blob, GCP Storage)\n",
        "  * Parquet/CSV/JSON files for downstream analytics\n",
        "* **Goal**: Make sure data is **accessible, fast to read, and reliable**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Advanced Concepts Often Used in ETL**\n",
        "\n",
        "Even in big service-based companies, ETL is more than just clean and save:\n",
        "\n",
        "* **Aggregations & Metrics** → Department-level salaries, counts, averages.\n",
        "* **Window Functions** → Rank employees by salary, running totals, moving averages.\n",
        "* **UDFs (User Defined Functions)** → Custom business logic, like category labels.\n",
        "* **Partitioning** → Split data into chunks for faster reading.\n",
        "* **Streaming ETL** → Continuous updates like live attendance or transactions.\n",
        "* **Monitoring & Logging** → Track failures, row counts, performance, alert on errors.\n",
        "\n",
        "---\n",
        "\n",
        "## **End-to-End Picture**\n",
        "\n",
        "1. **Extract** → Pull raw data from everywhere\n",
        "2. **Transform** → Clean, fix, standardize, enrich, aggregate\n",
        "3. **Load** → Save cleaned data for use by analysts, dashboards, or ML\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o8Le9UQuNerh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MrWi-SJEKzo2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EmployeeETL\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
        "spark = SparkSession.builder.appName(\"ETL_from_scratch\").getOrCreate()\n",
        "schema_emp = StructType([\n",
        "    StructField(\"EmpID\", StringType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"Salary\", DoubleType(), True),\n",
        "    StructField(\"DeptID\", StringType(), True),\n",
        "    StructField(\"Phone\", StringType(), True),\n",
        "    StructField(\"JoinDate\", StringType(), True)\n",
        "])\n",
        "schema_dept = StructType([\n",
        "    StructField(\"DeptID\", StringType(), True),\n",
        "    StructField(\"DeptName\", StringType(), True),\n",
        "    StructField(\"Location\", StringType(), True)\n",
        "])\n"
      ],
      "metadata": {
        "id": "ty3lIIVZLRWr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_emp = [\n",
        "    (\"E1\", \"Alice\", 30, 60000.0, \"D1\", \"9876543210\", \"2020-01-10\"),\n",
        "    (\"E2\", \"Bob\", 40, 75000.0, \"D2\", \"9876500000\", \"2019-03-15\"),\n",
        "    (\"E3\", \"Cathy\", None, None, \"D3\", \"123-456-7890\", \"2021-05-20\"),\n",
        "    (\"E4\", \"David\", 28, 90000.0, \"D2\", None, \"2022-07-25\"),\n",
        "    (\"E5\", \"Eva\", 35, 80000.0, \"D5\", \"9999999999\", \"invalid_date\"),\n",
        "]\n",
        "\n",
        "data_dept = [\n",
        "    (\"D1\", \"HR\", \"Delhi\"),\n",
        "    (\"D2\", \"IT\", \"Mumbai\"),\n",
        "    (\"D3\", \"Finance\", \"Bangalore\"),\n",
        "    (\"D4\", \"Marketing\", \"Chennai\")\n",
        "]\n",
        "\n",
        "\n",
        "df_emp = spark.createDataFrame(data_emp, schema_emp)\n",
        "df_dept = spark.createDataFrame(data_dept, schema_dept)\n",
        "\n",
        "print(\"=== Raw Employees ===\")\n",
        "df_emp.show(truncate=False)\n",
        "\n",
        "print(\"=== Raw Departments ===\")\n",
        "df_dept.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zt2bogSLRiB",
        "outputId": "615b8601-c515-47e5-fe05-6aa938f3db9d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Raw Employees ===\n",
            "+-----+-----+----+-------+------+------------+------------+\n",
            "|EmpID|Name |Age |Salary |DeptID|Phone       |JoinDate    |\n",
            "+-----+-----+----+-------+------+------------+------------+\n",
            "|E1   |Alice|30  |60000.0|D1    |9876543210  |2020-01-10  |\n",
            "|E2   |Bob  |40  |75000.0|D2    |9876500000  |2019-03-15  |\n",
            "|E3   |Cathy|NULL|NULL   |D3    |123-456-7890|2021-05-20  |\n",
            "|E4   |David|28  |90000.0|D2    |NULL        |2022-07-25  |\n",
            "|E5   |Eva  |35  |80000.0|D5    |9999999999  |invalid_date|\n",
            "+-----+-----+----+-------+------+------------+------------+\n",
            "\n",
            "=== Raw Departments ===\n",
            "+------+---------+---------+\n",
            "|DeptID|DeptName |Location |\n",
            "+------+---------+---------+\n",
            "|D1    |HR       |Delhi    |\n",
            "|D2    |IT       |Mumbai   |\n",
            "|D3    |Finance  |Bangalore|\n",
            "|D4    |Marketing|Chennai  |\n",
            "+------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, regexp_replace, to_date, lit\n",
        "\n",
        "avg_salary = df_emp.selectExpr(\"avg(Salary)\").first()[0]\n",
        "\n",
        "df_emp_clean = df_emp.withColumn(\n",
        "    \"Salary\",\n",
        "    when(col(\"Salary\").isNull(), avg_salary).otherwise(col(\"Salary\"))\n",
        ")\n",
        "df_emp_clean = df_emp_clean.withColumn(\n",
        "    \"Phone\",\n",
        "    when(col(\"Phone\").isNull(), lit(\"Unknown\")).otherwise(col(\"Phone\"))\n",
        ")\n",
        "df_emp_clean = df_emp_clean.withColumn(\"Phone\", regexp_replace(\"Phone\", \"-\", \"\"))\n",
        "df_emp_clean = df_emp_clean.withColumn(\"JoinDate\", to_date(\"JoinDate\", \"yyyy-MM-dd\"))\n"
      ],
      "metadata": {
        "id": "Cu70S3OULRk2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined = df_emp_clean.join(df_dept, on=\"DeptID\", how=\"left\")\n",
        "\n",
        "print(\"=== After Join ===\")\n",
        "df_joined.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phsdU15nLRnH",
        "outputId": "ec74fb7e-ed54-454f-f436-5997fa7b33a5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== After Join ===\n",
            "+------+-----+-----+----+-------+----------+----------+--------+---------+\n",
            "|DeptID|EmpID|Name |Age |Salary |Phone     |JoinDate  |DeptName|Location |\n",
            "+------+-----+-----+----+-------+----------+----------+--------+---------+\n",
            "|D1    |E1   |Alice|30  |60000.0|9876543210|2020-01-10|HR      |Delhi    |\n",
            "|D2    |E2   |Bob  |40  |75000.0|9876500000|2019-03-15|IT      |Mumbai   |\n",
            "|D5    |E5   |Eva  |35  |80000.0|9999999999|NULL      |NULL    |NULL     |\n",
            "|D3    |E3   |Cathy|NULL|76250.0|1234567890|2021-05-20|Finance |Bangalore|\n",
            "|D2    |E4   |David|28  |90000.0|Unknown   |2022-07-25|IT      |Mumbai   |\n",
            "+------+-----+-----+----+-------+----------+----------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, max, min, count\n",
        "df_agg = df_joined.groupBy(\"DeptName\").agg(\n",
        "    avg(\"Salary\").alias(\"AvgSalary\"),\n",
        "    max(\"Salary\").alias(\"MaxSalary\"),\n",
        "    min(\"Salary\").alias(\"MinSalary\"),\n",
        "    count(\"EmpID\").alias(\"EmployeeCount\")\n",
        ")\n",
        "\n",
        "print(\"=== Department Salary Stats ===\")\n",
        "df_agg.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csJy3dvKLRpU",
        "outputId": "db5ac5a2-a533-4a1f-f0da-a99afc565795"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Department Salary Stats ===\n",
            "+--------+---------+---------+---------+-------------+\n",
            "|DeptName|AvgSalary|MaxSalary|MinSalary|EmployeeCount|\n",
            "+--------+---------+---------+---------+-------------+\n",
            "|HR      |60000.0  |60000.0  |60000.0  |1            |\n",
            "|NULL    |80000.0  |80000.0  |80000.0  |1            |\n",
            "|Finance |76250.0  |76250.0  |76250.0  |1            |\n",
            "|IT      |82500.0  |90000.0  |75000.0  |2            |\n",
            "+--------+---------+---------+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import collect_list, explode\n",
        "\n",
        "df_collected = df_joined.groupBy(\"DeptName\").agg(\n",
        "    collect_list(\"Name\").alias(\"Employees\")\n",
        ")\n",
        "\n",
        "df_skills = spark.createDataFrame([\n",
        "    (\"E1\", [\"Python\", \"Java\"]),\n",
        "    (\"E2\", [\"C++\", \"Go\"]),\n",
        "    (\"E3\", [\"Excel\"]),\n",
        "], [\"EmpID\", \"Skills\"])\n",
        "\n",
        "df_skills_exploded = df_skills.withColumn(\"Skill\", explode(\"Skills\"))\n"
      ],
      "metadata": {
        "id": "pxwVbSy3LRr2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_joined.write.mode(\"overwrite\").parquet(\"CleanedEmployees\")\n",
        "df_agg.write.mode(\"overwrite\").parquet(\"DeptAggregates\")\n"
      ],
      "metadata": {
        "id": "EBhMBSZvLRt-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_loaded = spark.read.parquet(\"CleanedEmployees\")\n",
        "df_loaded.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g0Yb6L_LRxb",
        "outputId": "eb9c26f2-864c-49a7-a360-1ffabdaa8edb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+----+-------+----------+----------+--------+---------+\n",
            "|DeptID|EmpID| Name| Age| Salary|     Phone|  JoinDate|DeptName| Location|\n",
            "+------+-----+-----+----+-------+----------+----------+--------+---------+\n",
            "|    D1|   E1|Alice|  30|60000.0|9876543210|2020-01-10|      HR|    Delhi|\n",
            "|    D2|   E2|  Bob|  40|75000.0|9876500000|2019-03-15|      IT|   Mumbai|\n",
            "|    D5|   E5|  Eva|  35|80000.0|9999999999|      NULL|    NULL|     NULL|\n",
            "|    D3|   E3|Cathy|NULL|76250.0|1234567890|2021-05-20| Finance|Bangalore|\n",
            "|    D2|   E4|David|  28|90000.0|   Unknown|2022-07-25|      IT|   Mumbai|\n",
            "+------+-----+-----+----+-------+----------+----------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}