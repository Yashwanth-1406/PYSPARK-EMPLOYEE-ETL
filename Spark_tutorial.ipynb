{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdb5KaALrIwdNpDHi42ICf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashwanth-1406/PYSPARK-EMPLOYEE-ETL/blob/main/Spark_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why Spark?**\n",
        "\n",
        "Spark is so much faster as compared to other softwares that provides the same service.\n",
        "\n",
        "And to be precise, it's around 10 to 100 times faster than all of its competitors.\n",
        "\n",
        "Spark ecosystem and its underlying structure makes it capable to achieve such high speed.\n",
        "\n",
        "Next is its distributed behavior.\n",
        "\n",
        "So spark works in distributed fashion, which means that it's not mandatory that you must have all your\n",
        "\n",
        "data on one machine for spark to read it.\n",
        "\n",
        "Or you can only use single machine processing power.\n",
        "\n",
        "Spark allows to do everything in distributed way.\n",
        "\n",
        "You can have, uh, you can have your file present in chunks in different locations, and you can use\n",
        "\n",
        "multiple clusters or nodes or processors to do the computations in parallel.\n",
        "\n",
        "Next one is the advanced analytics.\n",
        "\n",
        "So spark provides us to do the advanced analytics due to its different libraries including spark SQL,\n",
        "\n",
        "spark Machine Learning and graphics.\n",
        "\n",
        "Thus comprising of all these and many more, spark allows us to perform advanced analytics at any level.\n",
        "\n",
        "Nextbest point of spark is its real time analysis.\n",
        "\n",
        "So, uh, spark can do processing on streaming data as well.\n",
        "\n",
        "So instead of having a complete data before running the job, like we do in usually do in batch mode.\n",
        "\n",
        "Spark allows you to configure input stream on which data will be coming on regular basis on real time,\n",
        "\n",
        "and spark will process it using spark streaming library.\n",
        "\n",
        "Next one is actually powerful caching mechanism.\n",
        "\n",
        "So as I mentioned, that spark allows us to work in distributed fashion.\n",
        "\n",
        "So it there can be some cases where we need to access the data again and again from some storage.\n",
        "\n",
        "And we will see later that once we consume some data from this spark, spark wipes it out here.\n",
        "\n",
        "Spark provides with the caching facility, and with that you can cache the data and thus eliminate the\n",
        "\n",
        "overhead of computing data again.\n",
        "\n",
        "Next very interesting feature of spark is fault tolerant.\n",
        "\n",
        "The underlying structure of spark makes it fault tolerant.\n",
        "\n",
        "If any cluster or node uh that the spark is using goes down this this spark actually won't stop working.\n",
        "\n",
        "It will simply, uh, divide its load.\n",
        "\n",
        "It will simply divide it, uh, computation on a new executor, on a new cluster and then start working\n",
        "\n",
        "from there.\n",
        "\n",
        "Last, and actually, ah, relief for the DevOps guys is the ease of deployment of the spark.\n",
        "\n",
        "So spark supports its number of APIs and functionality in some of the most commonly used programming\n",
        "\n",
        "languages like Python, Scala, Java and R.\n",
        "\n",
        "So we just need to have the basic dependencies met at any server at any cluster.\n",
        "\n",
        "And spark is good to go on that.\n",
        "\n",
        "So these are the most important features of spark.\n",
        "\n"
      ],
      "metadata": {
        "id": "p9mxhPpDWb_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basics About hadoop ecosystem-**\n",
        "\n",
        "\n",
        "\n",
        "Hadoop is just uh, for the simplicity sake, you can consider Hadoop as a simple ecosystem that\n",
        "\n",
        "provides a underlying structure for MapReduce.\n",
        "\n",
        "So the the core components of Hadoop ecosystem includes the HDFs, the Hadoop distributed file storage\n",
        "\n",
        "that that actually manages the distributed file storage behavior of the spark.\n",
        "\n",
        "Next one is this yarn.\n",
        "\n",
        "So for the yarn, yet another resource, you can consider the yarn as the operating system.\n",
        "\n",
        "Just like what your operating system does that it uh, it go for the free resources it allocates, the\n",
        "\n",
        "resources, it, uh, it takes the resources back and all that stuff.\n",
        "\n",
        "The yarn manages the same thing in the Hadoop ecosystem.\n",
        "\n",
        "Over here is the MapReduce.\n",
        "\n",
        "So MapReduce is simply a technique for mapping and reducing the data, if I may take you over here.\n",
        "\n",
        "So let's just consider that.\n",
        "\n",
        "We have, uh, we have some sort of input data, and then we can map that data to some different sort\n",
        "\n",
        "of output.\n",
        "\n",
        "And then after mapping, we will reduce that data.\n",
        "\n",
        "We will reduce different map data to uh, further uh outputs.\n",
        "\n",
        "And then from that reduced data we get our output.\n",
        "\n",
        "So this is how uh, MapReduce basically works.\n",
        "\n",
        "Now if we go back over here.\n",
        "\n",
        "So this is exactly the same mechanism.\n",
        "\n",
        "It.\n",
        "\n",
        "At the bottom we have the distributed storage.\n",
        "\n",
        "On the top we have the yarn ecosystem, we have the yarn operating system.\n",
        "\n",
        "And at the top of all this we have a MapReduce.\n",
        "\n",
        "So previously before spark, uh, the Hadoop ecosystem was there.\n",
        "\n",
        "And usually people, uh, the developers were using this MapReduce technique.\n",
        "\n",
        "They were actually forced to write their code in this fashion for making it run in a distributed way\n",
        "\n",
        "and in a much faster way.\n",
        "\n",
        "But there was some limitations writing the code in this fashion and, uh, map.\n",
        "\n",
        "Firstly, driving that, okay, this thing will be mapped on that, and then I want to reshuffle it\n",
        "\n",
        "and then I want to reduce it.\n",
        "\n",
        "Actually, this is a pretty simple, uh, diagram of MapReduce, but there comes a number of other stuff\n",
        "\n",
        "for reshuffling and for reducing and all other math.\n",
        "\n",
        "So it's it was a great mess at that time.\n",
        "\n",
        "So to overcome all the hurdles, overcome all the manual management, this spark kicks in.\n",
        "\n",
        "Though you can consider spark as an alternate of MapReduce, and the underlying structure of spark is\n",
        "\n",
        "also the MapReduce.\n",
        "\n",
        "There are mappers, there are reducers.\n",
        "\n",
        "But how they do that, how how they are working this, how they are building this under the hood.\n",
        "\n",
        "It's an it's an abstract level.\n",
        "\n",
        "We don't really need to worry about that and we don't really need to consider it.\n",
        "\n",
        "Even this spark manages all of this stuff on its own.\n",
        "\n",
        "And spark is built on Hadoop ecosystem.\n",
        "\n",
        "So spark Act, the yarn actually uses this part.\n",
        "\n",
        "And this spark uh, also uses the HDFs for storing for actually getting the data from different storages\n",
        "\n",
        "and storing the output in different storages.\n",
        "\n",
        "So this was just the basic overview of Hadoop ecosystem, uh, which comprises of uh distributed storage,\n",
        "\n",
        "the OS and then the MapReduce which uses these two things for uh, making it work for making uh, for\n",
        "\n",
        "making uh, program work in a distributed fashion.\n",
        "\n",
        "Due to several, uh, complexities and several issues, this spark kicks in and spark is just an alternate\n",
        "\n",
        "of MapReduce.\n",
        "\n",
        "It also uses the yarn.\n",
        "\n",
        "It also uses the HDFs, and it provides a much more reliable, much more faster and much more durable\n",
        "\n",
        "MapReduce functionality.\n",
        "\n",
        "So in this video, uh, I just gave you a kind of high level overview of Hadoop ecosystem, uh, because\n",
        "\n",
        "we, we are not going in depth of Hadoop ecosystem.\n",
        "\n",
        "So it's actually useless to provide you, uh, in depth or in depth idea of each and every component\n",
        "\n",
        "because there are other number of components available.\n"
      ],
      "metadata": {
        "id": "2PhwbIKGWcCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Spark Architecture and Spark ecoSystem?**\n",
        "\n",
        "The first one is the spark context or the driver node.\n",
        "\n",
        "That, uh, that actually contains the code.\n",
        "\n",
        "So, uh, your code is present inside the driver node, and it is responsible for managing all of or\n",
        "\n",
        "managing all the data and managing all the code.\n",
        "\n",
        "Next one is the cluster manager.\n",
        "\n",
        "So as I mentioned that this spark works in a distributed fashion.\n",
        "\n",
        "So there must be different workers available for the spark to work.\n",
        "\n",
        "Now let's say if we are working a spark on a local machine, uh, let's just consider a simple example.\n",
        "\n",
        "So on my local machine, let's say I have five cores on my machine.\n",
        "\n",
        "So spark will allow me to create five worker nodes.\n",
        "\n",
        "And then it will distribute the processing, distribute all the computation amongst the five cores,\n",
        "\n",
        "and they will run them in parallel.\n",
        "\n",
        "If I'm working on a cluster, uh, where there are different, I can create different, uh, executors\n",
        "\n",
        "and I can create different nodes.\n",
        "\n",
        "So here the cluster manager kicks in that will manage that will manage all the workers that okay this\n",
        "\n",
        "worker is free.\n",
        "\n",
        "Allocate this task to this worker okay.\n",
        "\n",
        "This worker has completed the task okay.\n",
        "\n",
        "Take the data, take the computation out and provide it with this.\n",
        "\n",
        "Okay.\n",
        "\n",
        "Then uh, if the if this worker is getting slower, distributed its task with the other worker, all\n",
        "\n",
        "of this thing is, is actually managed by this cluster manager.\n",
        "\n",
        "Next one is the worker.\n",
        "\n",
        "So worker, uh, you must know that the workers are responsible for doing the actual work, for doing\n",
        "\n",
        "the actual computation.\n",
        "\n",
        "They are not aware of the context.\n",
        "\n",
        "They are not aware of the actual, uh, what's happening as a big image.\n",
        "\n",
        "They just get their work that okay, you need to get this data and you need to process it in this fashion\n",
        "\n",
        "and then you need to provide this output.\n",
        "\n",
        "They just get this input.\n",
        "\n",
        "They do whatever they get and then they provide the final output.\n",
        "\n",
        "So this is how the spark architecture works.\n",
        "\n",
        "Again I'm repeating the same thing that we don't really need to worry about anything.\n",
        "\n",
        "That how spark how the cluster manager works, how spark context work and how the workers actually,\n",
        "\n",
        "uh, sync with each other.\n",
        "\n",
        "All of this thing, all of this stuff are managed by spark on its own.\n",
        "\n",
        "We don't really need to worry about that.\n",
        "\n",
        "I'm just showing you all this stuff.\n",
        "\n",
        "Just to give you the idea of what?\n",
        "\n",
        "What's actually happening under the hood.\n",
        "\n",
        "Next one is the spark ecosystem.\n",
        "\n",
        "So the spark ecosystem consists of basically two things.\n",
        "\n",
        "The first one is the spark core APIs, uh, in which this in which we can access the spark.\n",
        "\n",
        "So spark is just like a basic, uh, basic, uh, program which is written in Scala and as after the\n",
        "\n",
        "after the developers, after the community saw the popularity of this spark, the potential of this\n",
        "\n",
        "spark, they introduced different languages.\n",
        "\n",
        "So now the core is there, the core, uh, the core spark, which is written in Scala, is always there.\n",
        "\n",
        "They provided different APIs.\n",
        "\n",
        "So we can we can write our code in Java, and that will hit on the same core and that will provide the\n",
        "\n",
        "same output.\n",
        "\n",
        "We can again write our code in Scala, and then we can hit on the same core and we can get the same\n",
        "\n",
        "results.\n",
        "\n",
        "Same for the Python, same for the R.\n",
        "\n",
        "So what.\n",
        "\n",
        "That's the beauty of spark that it doesn't compels you to use a single language.\n",
        "\n",
        "Instead, they provide multiple core APIs that we can write our code in Java, we can write our code\n",
        "\n",
        "in Scala, we can write our code in Python, or we can write our code in R.\n",
        "\n",
        "And that's an abstract level.\n",
        "\n",
        "The core is always there, which will communicate with all of these languages and provide you with whatever\n",
        "\n",
        "you want.\n",
        "\n",
        "Next is the important libraries that are that actually this spark provides for, uh, working in a much\n",
        "\n",
        "better fashion.\n",
        "\n",
        "So just imagine, while working with the MapReduce that I gave you the previous image of Hadoop ecosystem.\n",
        "\n",
        "So MapReduce was there, but that was only responsible for doing the MapReduce.\n",
        "\n",
        "Now just imagine that if if I if a person wants to do some machine learning, uh, code, a machine\n",
        "\n",
        "learning model, he wants to write an algorithm or he wants to do some complex processing, then he\n",
        "\n",
        "will eventually, uh, stuck it with all the MapReduce logic.\n",
        "\n",
        "And that will be actually slow because a MapReduce for each iteration of the logic, for each iteration\n",
        "\n",
        "of the algorithm to work, he has to revisit the MapReduce system.\n",
        "\n",
        "So that's that's kind of a messy here in spark ecosystem, the spark provides a number of libraries\n",
        "\n",
        "which will, under the hood, use this power core APIs.\n",
        "\n",
        "And we don't really need to worry about that.\n",
        "\n",
        "So the first one is spark SQL that allows you to consider the data as a sequel table, so you can query\n",
        "\n",
        "on the data that that's a big catch.\n",
        "\n",
        "That's a big hit for this spark.\n",
        "\n",
        "Next one is the spark streaming.\n",
        "\n",
        "So here that's that's actually a thing that was not even imagined in Sci at the time of simple MapReduce,\n",
        "\n",
        "that instead of working on the batch data, instead of having some file at the at the beginning and\n",
        "\n",
        "then reading the file and then providing the output, this spark streaming allows you to configure some\n",
        "\n",
        "input stream with the code.\n",
        "\n",
        "Now let's say for input stream examples, you can consider Twitter, you can consider Facebook, or\n",
        "\n",
        "you can consider Reddit, any any website which is live, any stream which is live.\n",
        "\n",
        "Uh, or you can go for AWS Kinesis and all that stuff.\n",
        "\n",
        "So spark allows you to take some data from the streaming, from some streaming inputs, and then it\n",
        "\n",
        "allows you to do the processing on it and updates the output real time.\n",
        "\n",
        "Next one is the spark Mllib machine learning library.\n",
        "\n",
        "So spark provides a number of algorithms that are available inside, uh, the spark Mllib, and the\n",
        "\n",
        "developers can use them to train their, uh, model, uh, the, uh, to write their algorithms, to\n",
        "\n",
        "implement their algorithms.\n",
        "\n",
        "And they can use data to train their models inside this spark.\n",
        "\n",
        "Mllib next one is this, uh, spark graphics.\n",
        "\n",
        "So, uh, if you are not familiar with graphics or graphics are like, uh, graph notation of the data,\n",
        "\n",
        "and it allows you to simply create nodes and vertices, and then it allows you to run algorithms on\n",
        "\n",
        "different nodes on different, uh, vertices based on different condition.\n",
        "\n",
        "So this is what the spark ecosystem is that it has some core APIs.\n",
        "\n",
        "It has some libraries.\n",
        "\n",
        "And under the hood at the bottom of everything there is core spark that is actually come that with which\n",
        "\n",
        "all of these things are communicating and providing you with the result.\n",
        "\n",
        "So in this video, I just gave you the overview of Spark Architecture and Spark ecosystem.\n",
        "\n",
        "I am I really hope that you are getting along with all of this stuff.\n",
        "\n",
        "But one thing more I want to add over here that if you are not getting anything right now, then don't\n",
        "\n",
        "worry, this is just the understanding of what's happening under the hood.\n",
        "\n",
        "Later on, when we will start doing hands on with that, we will start, uh, reading something, processing\n",
        "\n",
        "something, writing something.\n",
        "\n",
        "You will eventually develop the understanding.\n",
        "\n",
        "And once again, we don't.\n",
        "\n",
        "Really need to manage anything.\n",
        "\n",
        "We don't really have to manage anything.\n",
        "\n",
        "Spark and its.\n",
        "\n",
        "Its underlying Hadoop ecosystem manages each and everything on its own.\n"
      ],
      "metadata": {
        "id": "S8aV7TJjWcFu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgiubiSacme4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vt1ZdYOIcmqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cPumHVgbcmty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}